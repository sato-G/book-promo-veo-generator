#!/usr/bin/env python3
"""
å†’é ­ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

æ›¸ç±ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³å‹•ç”»ã®å†’é ­ç”¨ã®å›è»¢ã‚ºãƒ¼ãƒ ãƒãƒƒã‚¯ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³
360åº¦å›è»¢ã—ãªãŒã‚‰ã‚ºãƒ¼ãƒ ãƒãƒƒã‚¯ã™ã‚‹è¿«åŠ›ã®ã‚ã‚‹æ¼”å‡º
"""

from pathlib import Path
from typing import Optional, Tuple
from PIL import Image, ImageDraw, ImageFont
import numpy as np
import tempfile
from moviepy import ImageClip, CompositeVideoClip, VideoClip, AudioFileClip


def generate_opening_animation(
    image_path: Path,
    output_path: Path,
    catchphrase: Optional[str] = None,
    duration: float = 2.0,  # åˆè¨ˆ2ç§’ï¼ˆ0.2ç§’ã‚¢ãƒ‹ãƒ¡ + 1.8ç§’åœæ­¢ï¼‰
    animation_duration: float = 0.2,  # ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³éƒ¨åˆ†ï¼ˆ0.2ç§’ï¼‰
    zoom_start: float = 2.5,
    zoom_end: float = 1.0,
    resolution: Tuple[int, int] = (1080, 1920),
    fps: int = 30,
    enable_tts: bool = False  # TTSãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æœ‰åŠ¹åŒ–
) -> Path:
    """
    å†’é ­ã®å›è»¢ã‚ºãƒ¼ãƒ ãƒãƒƒã‚¯ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³å‹•ç”»ã‚’ç”Ÿæˆ

    Args:
        image_path: å…¥åŠ›ç”»åƒãƒ‘ã‚¹ï¼ˆæ›¸ç±ã®è¡¨ç´™ãªã©ï¼‰
        output_path: å‡ºåŠ›å‹•ç”»ãƒ‘ã‚¹
        catchphrase: ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼ï¼ˆå­—å¹•ã¨ã—ã¦ç”»åƒä¸­å¤®ã«è¡¨ç¤ºï¼‰
        duration: å‹•ç”»ã®é•·ã•ï¼ˆç§’ï¼‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ0.2ç§’
        zoom_start: é–‹å§‹æ™‚ã®æ‹¡å¤§ç‡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ2.5å€ï¼‰
        zoom_end: çµ‚äº†æ™‚ã®æ‹¡å¤§ç‡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1.0å€ï¼‰
        resolution: å‹•ç”»è§£åƒåº¦ (width, height)
        fps: ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ¼ãƒˆ

    Returns:
        å‡ºåŠ›å‹•ç”»ã®ãƒ‘ã‚¹
    """
    print("=" * 60)
    print("ğŸ¬ å†’é ­ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆé–‹å§‹")
    print("=" * 60)
    print(f"ğŸ“¸ å…¥åŠ›ç”»åƒ: {image_path}")
    print(f"â±ï¸  å‹•ç”»ã®é•·ã•: {duration}ç§’")
    print(f"ğŸ” ã‚ºãƒ¼ãƒ : {zoom_start}x â†’ {zoom_end}x")
    if catchphrase:
        print(f"ğŸ’¬ ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼: {catchphrase}")
    print("=" * 60 + "\n")

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # ==========================================
    # 1. ç”»åƒã‚’èª­ã¿è¾¼ã‚“ã§ãƒªã‚µã‚¤ã‚º
    # ==========================================
    print("ã€1ã€‘ç”»åƒã‚’èª­ã¿è¾¼ã¿ä¸­...")
    img = Image.open(image_path)

    # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‚’ä¿ã¡ãªãŒã‚‰ãƒªã‚µã‚¤ã‚º
    img_w, img_h = img.size
    target_w, target_h = resolution

    # ç”»åƒãŒç¸¦é•·ï¼ˆ9:16ï¼‰ã‹æ¨ªé•·ï¼ˆ16:9ï¼‰ã‹ã‚’åˆ¤å®š
    img_aspect = img_w / img_h
    target_aspect = target_w / target_h

    if img_aspect > target_aspect:
        # ç”»åƒãŒæ¨ªé•· â†’ é«˜ã•åŸºæº–ã§ãƒªã‚µã‚¤ã‚º
        new_h = target_h
        new_w = int(img_w * (target_h / img_h))
    else:
        # ç”»åƒãŒç¸¦é•· â†’ å¹…åŸºæº–ã§ãƒªã‚µã‚¤ã‚º
        new_w = target_w
        new_h = int(img_h * (target_w / img_w))

    img_resized = img.resize((new_w, new_h), Image.Resampling.LANCZOS)
    print(f"âœ“ ç”»åƒãƒªã‚µã‚¤ã‚ºå®Œäº†: {img_w}x{img_h} â†’ {new_w}x{new_h}")

    # ==========================================
    # 2. å­—å¹•ä»˜ãç”»åƒã‚’ä½œæˆ
    # ==========================================
    print("\nã€2ã€‘å­—å¹•ä»˜ãç”»åƒã‚’ä½œæˆä¸­...")

    # å…ƒç”»åƒã«å­—å¹•ã‚’ç„¼ãè¾¼ã‚€
    base_img_with_subtitle = img_resized.copy()

    if catchphrase:
        print(f"   å­—å¹•ã‚’ç”»åƒã«ç„¼ãè¾¼ã¿ä¸­: {catchphrase}")
        # RGBç”»åƒã«å¤‰æ›
        if base_img_with_subtitle.mode != 'RGB':
            base_img_with_subtitle = base_img_with_subtitle.convert('RGB')

        draw = ImageDraw.Draw(base_img_with_subtitle)

        # ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆãƒ’ãƒ©ã‚®ãƒãƒ•ã‚©ãƒ³ãƒˆ - å¤§ããï¼‰
        fontsize = 100  # 60 â†’ 100ï¼ˆã‚ˆã‚Šå¤§ããï¼‰
        try:
            font = ImageFont.truetype("/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒè§’ã‚´ã‚·ãƒƒã‚¯ W6.ttc", fontsize)
        except:
            try:
                font = ImageFont.truetype("/System/Library/Fonts/Hiragino Sans GB.ttc", fontsize)
            except:
                font = ImageFont.load_default()

        # 2è¡Œã«åˆ†å‰²ï¼ˆ10æ–‡å­—ä»¥ä¸Šãªã‚‰ä¸­å¤®ã§æ”¹è¡Œï¼‰
        lines = []
        if len(catchphrase) >= 10:
            # ä¸­å¤®ã‚ãŸã‚Šã§åˆ†å‰²
            mid = len(catchphrase) // 2
            # åŒºåˆ‡ã‚Šæ–‡å­—ã§åˆ†å‰²ã‚’è©¦ã¿ã‚‹
            best_split = mid
            for i in range(mid - 3, mid + 4):
                if i > 0 and i < len(catchphrase):
                    if catchphrase[i] in ['ã€', 'ã€‚', 'ï¼', 'ï¼Ÿ', '?', ' ']:
                        best_split = i + 1
                        break
            lines = [catchphrase[:best_split].strip(), catchphrase[best_split:].strip()]
        else:
            lines = [catchphrase]

        # å„è¡Œã®ã‚µã‚¤ã‚ºã‚’è¨ˆç®—
        line_heights = []
        max_width = 0
        for line in lines:
            bbox = draw.textbbox((0, 0), line, font=font)
            line_width = bbox[2] - bbox[0]
            line_height = bbox[3] - bbox[1]
            line_heights.append(line_height)
            max_width = max(max_width, line_width)

        # è¡Œé–“
        line_spacing = 20
        total_height = sum(line_heights) + line_spacing * (len(lines) - 1)

        # ä½ç½®ã‚’è¨ˆç®—ï¼ˆç”»é¢ä¸­å¤®ï¼‰
        start_y = (new_h - total_height) // 2

        # å„è¡Œã‚’æç”»
        current_y = start_y
        for i, line in enumerate(lines):
            bbox = draw.textbbox((0, 0), line, font=font)
            text_width = bbox[2] - bbox[0]
            x = (new_w - text_width) // 2

            # é»’ã„ç¸å–ã‚Šï¼ˆã•ã‚‰ã«å¤ªãï¼‰
            for offset_x in range(-6, 7):
                for offset_y in range(-6, 7):
                    if offset_x != 0 or offset_y != 0:
                        draw.text((x + offset_x, current_y + offset_y), line, font=font, fill=(0, 0, 0))

            # ãƒ†ã‚­ã‚¹ãƒˆã‚’æç”»ï¼ˆé»„è‰² - ã‚´ãƒ¼ãƒ«ãƒ‰ï¼‰
            draw.text((x, current_y), line, font=font, fill=(255, 215, 0))

            current_y += line_heights[i] + line_spacing

        print(f"   âœ“ å­—å¹•ç„¼ãè¾¼ã¿å®Œäº†ï¼ˆ{len(lines)}è¡Œã€é»„è‰²ã€ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚º{fontsize}ï¼‰")

    # ç”»åƒã‚’numpyé…åˆ—ã«å¤‰æ›
    img_array = np.array(base_img_with_subtitle)

    # ==========================================
    # 3. ã‚ºãƒ¼ãƒ ãƒãƒƒã‚¯ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¯ãƒªãƒƒãƒ—ã‚’ä½œæˆ
    # ==========================================
    print("\nã€3ã€‘ã‚ºãƒ¼ãƒ ãƒãƒƒã‚¯ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½œæˆä¸­...")

    def make_frame(t):
        """æ™‚é–“tã«å¿œã˜ãŸãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ç”Ÿæˆï¼ˆå›è»¢ã—ãªãŒã‚‰ã‚ºãƒ¼ãƒ ãƒãƒƒã‚¯åŠ¹æœã€ãã®å¾Œåœæ­¢ï¼‰"""
        if t < animation_duration:
            # Phase 1: ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ (0 â†’ animation_duration)
            progress = t / animation_duration
            # ç¾åœ¨ã®ã‚ºãƒ¼ãƒ ç‡ï¼ˆæ‹¡å¤§ â†’ å…ƒã‚µã‚¤ã‚ºï¼‰
            current_zoom = zoom_start + (zoom_end - zoom_start) * progress
            # å›è»¢è§’åº¦ï¼ˆ360åº¦å›è»¢ï¼‰
            rotation_angle = 360 * progress
        else:
            # Phase 2: é™æ­¢ (animation_duration â†’ duration)
            # æœ€çµ‚ãƒ•ãƒ¬ãƒ¼ãƒ ã§åœæ­¢ï¼ˆæ­£é¢ã€æ‹¡å¤§ãªã—ï¼‰
            current_zoom = zoom_end  # 1.0x
            rotation_angle = 0  # æ­£é¢ï¼ˆå›è»¢ãªã—ï¼‰

        # ã‚ºãƒ¼ãƒ ã‚’é©ç”¨ã—ãŸç”»åƒã‚µã‚¤ã‚º
        zoomed_w = int(new_w * current_zoom)
        zoomed_h = int(new_h * current_zoom)

        # PIL Imageã§ãƒªã‚µã‚¤ã‚º
        zoomed_img = Image.fromarray(img_array).resize(
            (zoomed_w, zoomed_h),
            Image.Resampling.LANCZOS
        )

        # å›è»¢ã‚’é©ç”¨ï¼ˆexpand=Trueã§å›è»¢å¾Œã®ç”»åƒå…¨ä½“ã‚’å«ã‚€ï¼‰
        rotated_img = zoomed_img.rotate(
            -rotation_angle,  # æ™‚è¨ˆå›ã‚Š
            resample=Image.Resampling.BICUBIC,
            expand=True,
            fillcolor=(0, 0, 0)
        )

        # å›è»¢å¾Œã®ã‚µã‚¤ã‚º
        rot_w, rot_h = rotated_img.size

        # ä¸­å¤®ã‚¯ãƒ­ãƒƒãƒ—ã—ã¦ç›®æ¨™ã‚µã‚¤ã‚ºã«ã™ã‚‹
        left = (rot_w - target_w) // 2
        top = (rot_h - target_h) // 2

        # ã‚¯ãƒ­ãƒƒãƒ—ç¯„å›²ã‚’è¨ˆç®—
        crop_left = max(0, left)
        crop_top = max(0, top)
        crop_right = min(rot_w, left + target_w)
        crop_bottom = min(rot_h, top + target_h)

        cropped = rotated_img.crop((crop_left, crop_top, crop_right, crop_bottom))

        # é»’èƒŒæ™¯ã®ã‚­ãƒ£ãƒ³ãƒã‚¹ã‚’ä½œæˆ
        canvas = Image.new('RGB', resolution, (0, 0, 0))

        # ä¸­å¤®ã«é…ç½®
        paste_x = (target_w - cropped.width) // 2
        paste_y = (target_h - cropped.height) // 2
        canvas.paste(cropped, (paste_x, paste_y))

        return np.array(canvas)

    # ã‚«ã‚¹ã‚¿ãƒ ã‚¯ãƒªãƒƒãƒ—ã‚’ä½œæˆ
    final_clip = VideoClip(make_frame, duration=duration).with_fps(fps)

    print(f"âœ“ å›è»¢ã‚ºãƒ¼ãƒ ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆå®Œäº† ({zoom_start}x â†’ {zoom_end}x, 360åº¦å›è»¢)")

    # ==========================================
    # 4. ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³éŸ³å£°ã‚’ç”Ÿæˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    # ==========================================
    narration_audio = None
    if enable_tts and catchphrase:
        print("\nã€4ã€‘ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³éŸ³å£°ã‚’ç”Ÿæˆä¸­...")
        try:
            from src.generators.text_to_speech_client import TextToSpeechClient

            tts_client = TextToSpeechClient()
            result = tts_client.synthesize_speech(
                text=catchphrase,
                output_name="opening_narration",
                language_code="ja-JP",
                voice_name=tts_client.JAPANESE_VOICES["male_b"],  # ã‚ˆã‚Šä½ã„å£°
                voice_gender="MALE",
                speaking_rate=1.4,  # ã‚ˆã‚Šæ—©ã
                pitch=-8.0,  # ã‹ãªã‚Šä½ã‚
                volume_gain_db=3.0,
                output_dir=Path("data/output/speech")
            )

            if result['status'] == 'success':
                narration_audio = AudioFileClip(str(result['audio_file']))
                print(f"âœ“ ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³éŸ³å£°ç”Ÿæˆå®Œäº†: {narration_audio.duration:.2f}ç§’")

                # å‹•ç”»ã«éŸ³å£°ã‚’è¿½åŠ 
                final_clip = final_clip.with_audio(narration_audio)
            else:
                print("âš ï¸  ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³éŸ³å£°ç”Ÿæˆå¤±æ•—ï¼ˆéŸ³å£°ãªã—ã§ç¶šè¡Œï¼‰")
        except Exception as e:
            print(f"âš ï¸  ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³éŸ³å£°ç”Ÿæˆã‚¹ã‚­ãƒƒãƒ—: {e}")
            print("âœ“ éŸ³å£°ãªã—ã§ç¶šè¡Œã—ã¾ã™")

    # ==========================================
    # 5. å‹•ç”»ã‚’å‡ºåŠ›
    # ==========================================
    print("\nã€5ã€‘å‹•ç”»ã‚’å‡ºåŠ›ä¸­...")
    final_clip.write_videofile(
        str(output_path),
        fps=fps,
        codec='libx264',
        audio=(narration_audio is not None),  # éŸ³å£°ãŒã‚ã‚Œã°å«ã‚ã‚‹
        preset='medium',
        threads=4
    )

    print("\n" + "=" * 60)
    print("âœ… å†’é ­ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆå®Œäº†ï¼")
    print("=" * 60)
    print(f"ğŸ“‚ å‡ºåŠ›å…ˆ: {output_path}")
    print(f"â±ï¸  å‹•ç”»ã®é•·ã•: {duration}ç§’")
    print(f"ğŸ“ è§£åƒåº¦: {resolution[0]}x{resolution[1]}")
    print(f"ğŸ¬ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ¼ãƒˆ: {fps}fps")
    print(f"ğŸ™ï¸  ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: {'ã‚ã‚Š' if narration_audio else 'ãªã—'}")
    print("=" * 60)

    return output_path


def main():
    """CLIå®Ÿè¡Œç”¨ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='å†’é ­ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³å‹•ç”»ç”Ÿæˆ')
    parser.add_argument('--image', '-i', type=str, required=True,
                       help='å…¥åŠ›ç”»åƒãƒ‘ã‚¹')
    parser.add_argument('--output', '-o', type=str, required=True,
                       help='å‡ºåŠ›å‹•ç”»ãƒ‘ã‚¹')
    parser.add_argument('--catchphrase', '-c', type=str, default=None,
                       help='ã‚­ãƒ£ãƒƒãƒã‚³ãƒ”ãƒ¼ï¼ˆå­—å¹•ï¼‰')
    parser.add_argument('--duration', '-d', type=float, default=3.0,
                       help='å‹•ç”»ã®é•·ã•ï¼ˆç§’ï¼‰')
    parser.add_argument('--zoom-start', type=float, default=1.5,
                       help='é–‹å§‹æ™‚ã®æ‹¡å¤§ç‡')
    parser.add_argument('--zoom-end', type=float, default=1.0,
                       help='çµ‚äº†æ™‚ã®æ‹¡å¤§ç‡')

    args = parser.parse_args()

    generate_opening_animation(
        image_path=Path(args.image),
        output_path=Path(args.output),
        catchphrase=args.catchphrase,
        duration=args.duration,
        zoom_start=args.zoom_start,
        zoom_end=args.zoom_end
    )


if __name__ == '__main__':
    main()
