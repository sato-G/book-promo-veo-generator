#!/usr/bin/env python3
"""
ã€Œã‚ã®æˆ¦äº‰ã¯ä½•ã ã£ãŸã®ã‹ã€ç¸¦å‹å‹•ç”»ï¼ˆæ¨ªã‚¹ãƒ©ã‚¤ãƒ‰åˆ‡ã‚Šæ›¿ãˆï¼‰
- 12ç§’ã®å‹•ç”»
- ç¸¦å‹ï¼ˆ1080x1920ï¼‰
- æ¨ªã‚¹ãƒ©ã‚¤ãƒ‰ã§ç”»åƒåˆ‡ã‚Šæ›¿ãˆï¼ˆ4æšï¼‰
- ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åŒæœŸå­—å¹•
"""
from pathlib import Path
from moviepy import (
    ImageClip,
    CompositeVideoClip,
    AudioFileClip
)
from PIL import Image, ImageDraw, ImageFont
import tempfile
import numpy as np

# Text-to-Speechã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from text_to_speech_client import TextToSpeechClient


def create_slide_transition_clip(
    image_path: Path,
    duration: float,
    resolution: tuple = (1080, 1920),
    slide_direction: str = 'left'  # 'left' or 'right'
):
    """
    æ¨ªã‚¹ãƒ©ã‚¤ãƒ‰ãƒˆãƒ©ãƒ³ã‚¸ã‚·ãƒ§ãƒ³ + æ¨ªãƒ‘ãƒ³åŠ¹æœä»˜ãã®ç”»åƒã‚¯ãƒªãƒƒãƒ—ã‚’ä½œæˆ

    Args:
        image_path: ç”»åƒãƒ‘ã‚¹
        duration: è¡¨ç¤ºæ™‚é–“
        resolution: ç¸¦å‹è§£åƒåº¦ (width, height) = (1080, 1920)
        slide_direction: ã‚¹ãƒ©ã‚¤ãƒ‰æ–¹å‘ ('left' = å·¦ã‹ã‚‰å³ã¸, 'right' = å³ã‹ã‚‰å·¦ã¸)

    Returns:
        ã‚¹ãƒ©ã‚¤ãƒ‰åŠ¹æœ + ãƒ‘ãƒ³åŠ¹æœä»˜ãImageClip
    """
    target_width, target_height = resolution

    # ãƒ‘ãƒ³åŠ¹æœç”¨ã«å°‘ã—åºƒã‚ã®ç”»åƒã‚’ä½œæˆï¼ˆ1.15å€ã®å¹…ï¼‰
    pan_width = int(target_width * 1.15)
    max_pan_offset = pan_width - target_width

    # ç”»åƒã‚’èª­ã¿è¾¼ã‚“ã§ãƒªã‚µã‚¤ã‚º
    with Image.open(image_path) as img:
        # RGBå¤‰æ›
        if img.mode != 'RGB':
            img = img.convert('RGB')

        img_width, img_height = img.size
        img_aspect = img_width / img_height

        # ç¸¦å‹ã«ãƒ•ã‚£ãƒƒãƒˆï¼ˆé«˜ã•åŸºæº–ï¼‰
        new_height = target_height
        new_width = int(new_height * img_aspect)

        # ãƒ‘ãƒ³ç”¨ã®åºƒã‚ã®å¹…ã‚’ç¢ºä¿
        if new_width < pan_width:
            new_width = pan_width
            new_height = int(new_width / img_aspect)

        # ãƒªã‚µã‚¤ã‚º
        resized_img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)

        # ä¸­å¤®ã§ã‚¯ãƒ­ãƒƒãƒ—ï¼ˆåºƒã‚ã«ï¼‰
        left = (new_width - pan_width) // 2
        top = (new_height - target_height) // 2
        cropped_img = resized_img.crop((left, top, left + pan_width, top + target_height))

        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.jpg')
        cropped_img.save(temp_file.name, 'JPEG', quality=95)
        temp_file.close()

    # ãƒˆãƒ©ãƒ³ã‚¸ã‚·ãƒ§ãƒ³æ™‚é–“ï¼ˆ0.3ç§’ï¼‰
    transition_duration = 0.3

    def slide_and_pan_effect(get_frame, t):
        """ã‚¹ãƒ©ã‚¤ãƒ‰ã‚¤ãƒ³ + ãƒ‘ãƒ³åŠ¹æœ"""
        frame = get_frame(t)

        if t < transition_duration:
            # ã‚¹ãƒ©ã‚¤ãƒ‰ã‚¤ãƒ³ä¸­
            progress = t / transition_duration
            if slide_direction == 'left':
                # å·¦ã‹ã‚‰å³ã¸ã‚¹ãƒ©ã‚¤ãƒ‰ã‚¤ãƒ³
                x_offset = int(target_width * (1 - progress))
                result = np.zeros((target_height, target_width, 3), dtype=np.uint8)
                # åºƒã„ç”»åƒã®å·¦ç«¯ã‹ã‚‰åˆ‡ã‚Šå‡ºã—
                result[:, x_offset:] = frame[:, :target_width-x_offset]
                return result
            else:
                # å³ã‹ã‚‰å·¦ã¸ã‚¹ãƒ©ã‚¤ãƒ‰ã‚¤ãƒ³
                x_offset = int(target_width * (1 - progress))
                result = np.zeros((target_height, target_width, 3), dtype=np.uint8)
                # åºƒã„ç”»åƒã®å³ç«¯ã‹ã‚‰åˆ‡ã‚Šå‡ºã—
                result[:, :target_width-x_offset] = frame[:, max_pan_offset+x_offset:max_pan_offset+target_width]
                return result
        else:
            # ãƒ‘ãƒ³åŠ¹æœï¼ˆãƒˆãƒ©ãƒ³ã‚¸ã‚·ãƒ§ãƒ³å¾Œï¼‰
            pan_progress = (t - transition_duration) / (duration - transition_duration)

            if slide_direction == 'left':
                # å·¦ã‹ã‚‰å³ã¸ã‚†ã£ãã‚Šãƒ‘ãƒ³
                pan_offset = int(max_pan_offset * pan_progress)
                return frame[:, pan_offset:pan_offset+target_width]
            else:
                # å³ã‹ã‚‰å·¦ã¸ã‚†ã£ãã‚Šãƒ‘ãƒ³
                pan_offset = int(max_pan_offset * (1 - pan_progress))
                return frame[:, pan_offset:pan_offset+target_width]

    # åŸºæœ¬ã‚¯ãƒªãƒƒãƒ—ã‚’ä½œæˆ
    clip = ImageClip(temp_file.name).with_duration(duration)

    # ã‚¹ãƒ©ã‚¤ãƒ‰ + ãƒ‘ãƒ³åŠ¹æœã‚’é©ç”¨
    clip = clip.transform(slide_and_pan_effect)

    return clip


def create_subtitle_clip_vertical(
    text: str,
    start_time: float,
    duration: float,
    fontsize: int = 48,
    size: tuple = (1080, 1920)
):
    """
    ç¸¦å‹å‹•ç”»ç”¨ã®å­—å¹•ã‚¯ãƒªãƒƒãƒ—ã‚’ä½œæˆ

    Args:
        text: å­—å¹•ãƒ†ã‚­ã‚¹ãƒˆ
        start_time: é–‹å§‹æ™‚åˆ»
        duration: è¡¨ç¤ºæ™‚é–“
        fontsize: ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚º
        size: ç”»åƒã‚µã‚¤ã‚º

    Returns:
        ImageClip
    """
    # é€æ˜èƒŒæ™¯ã®ç”»åƒã‚’ä½œæˆ
    img = Image.new('RGBA', size, (0, 0, 0, 0))
    draw = ImageDraw.Draw(img)

    # ãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
    try:
        font = ImageFont.truetype("/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒè§’ã‚´ã‚·ãƒƒã‚¯ W6.ttc", fontsize)
    except:
        try:
            font = ImageFont.truetype("/System/Library/Fonts/Hiragino Sans GB.ttc", fontsize)
        except:
            font = ImageFont.load_default()

    # ãƒ†ã‚­ã‚¹ãƒˆã®å¢ƒç•Œãƒœãƒƒã‚¯ã‚¹ã‚’å–å¾—
    bbox = draw.textbbox((0, 0), text, font=font)
    text_width = bbox[2] - bbox[0]
    text_height = bbox[3] - bbox[1]

    # ä½ç½®ã‚’è¨ˆç®—ï¼ˆä¸‹éƒ¨ä¸­å¤®ï¼‰
    x = (size[0] - text_width) // 2
    y = size[1] - 250 - text_height

    # é»’ã„ç¸å–ã‚Šï¼ˆå¤ªã‚ï¼‰
    for offset_x in range(-4, 5):
        for offset_y in range(-4, 5):
            if offset_x != 0 or offset_y != 0:
                draw.text((x + offset_x, y + offset_y), text, font=font, fill=(0, 0, 0, 255))

    # ãƒ†ã‚­ã‚¹ãƒˆã‚’æç”»ï¼ˆç™½ï¼‰
    draw.text((x, y), text, font=font, fill=(255, 255, 255, 255))

    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.png')
    img.save(temp_file.name)
    temp_file.close()

    # ImageClipã¨ã—ã¦è¿”ã™
    return (ImageClip(temp_file.name, transparent=True)
            .with_duration(duration)
            .with_start(start_time))


def create_war_vertical_slide_12s(
    image_dir: str,
    output_path: str,
    narration_segments: list = None,
    bgm_path: str = None,
    duration: float = 12.0,
    resolution: tuple = (1080, 1920)
):
    """
    ç¸¦å‹å‹•ç”»ã‚’ç”Ÿæˆï¼ˆæ¨ªã‚¹ãƒ©ã‚¤ãƒ‰åˆ‡ã‚Šæ›¿ãˆï¼‰

    Args:
        image_dir: ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        output_path: å‡ºåŠ›ãƒ‘ã‚¹
        narration_segments: ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ
        bgm_path: BGMãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        duration: å‹•ç”»ã®é•·ã•
        resolution: è§£åƒåº¦ï¼ˆç¸¦å‹: 1080x1920ï¼‰
    """
    image_dir = Path(image_dir)

    print("=" * 60)
    print("ğŸ¬ ã€Œã‚ã®æˆ¦äº‰ã¯ä½•ã ã£ãŸã®ã‹ã€ç¸¦å‹å‹•ç”»ç”Ÿæˆï¼ˆæ¨ªã‚¹ãƒ©ã‚¤ãƒ‰ï¼‰")
    print("=" * 60)

    # ==========================================
    # 1. ç”»åƒã‚’é¸æŠï¼ˆ4æšï¼‰
    # ==========================================
    print("\nã€1ã€‘ç”»åƒã‚’æº–å‚™ä¸­...")

    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    image_files = []
    for i in range(1, 11):
        compressed_path = image_dir / f"AIç”¨ç´ æ_{i}_compressed.jpg"
        img_path = image_dir / f"AIç”¨ç´ æ_{i}.jpg"

        if compressed_path.exists():
            image_files.append(compressed_path)
        elif img_path.exists():
            image_files.append(img_path)

    if not image_files:
        raise ValueError(f"ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {image_dir}")

    # æœ€åˆã®4æšã‚’é¸æŠ
    selected_images = image_files[:4]

    print(f"âœ“ ä½¿ç”¨ã™ã‚‹ç”»åƒ: {len(selected_images)}æš")

    # ==========================================
    # 2. ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³éŸ³å£°ã‚’ç”Ÿæˆï¼ˆã‚»ã‚°ãƒ¡ãƒ³ãƒˆã”ã¨ï¼‰ã¾ãŸã¯ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãªã—ã§å­—å¹•ã®ã¿
    # ==========================================
    updated_narration_segments = []
    actual_duration = duration  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æŒ‡å®šã•ã‚ŒãŸé•·ã•

    if narration_segments:
        print("\nã€2ã€‘å­—å¹•ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’æº–å‚™ä¸­...")

        # ã¾ãšå­—å¹•ç”¨ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’æº–å‚™ï¼ˆéŸ³å£°ãªã—ï¼‰
        current_time = 0.0
        segment_duration = duration / len(narration_segments)

        for i, segment in enumerate(narration_segments):
            updated_segment = {
                'text': segment['text'],
                'start': current_time,
                'duration': segment_duration
            }
            updated_narration_segments.append(updated_segment)
            print(f"   ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ{i+1}: {segment_duration:.2f}ç§’ - {segment['text']}")
            current_time += segment_duration

        narration_segments = updated_narration_segments
        print(f"âœ“ å­—å¹•ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæº–å‚™å®Œäº†: {len(narration_segments)}å€‹")

        # éŸ³å£°åˆæˆã‚’è©¦ã¿ã‚‹ï¼ˆå¤±æ•—ã—ã¦ã‚‚å­—å¹•ã¯è¡¨ç¤ºã•ã‚Œã‚‹ï¼‰
        try:
            print("\nã€2-ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€‘ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³éŸ³å£°ã‚’ç”Ÿæˆä¸­...")
            tts_client = TextToSpeechClient()

            audio_segments = []
            for i, segment in enumerate(narration_segments):
                result = tts_client.synthesize_speech(
                    text=segment['text'],
                    output_name=f"war_narration_segment_{i+1}",
                    language_code="ja-JP",
                    voice_name=tts_client.JAPANESE_VOICES["male_a"],
                    voice_gender="MALE",
                    speaking_rate=1.2,
                    pitch=-5.0,
                    volume_gain_db=3.0,
                    output_dir=Path("data/output/speech")
                )

                if result['status'] == 'success':
                    audio_clip = AudioFileClip(str(result['audio_file']))
                    audio_segments.append(audio_clip)
                    print(f"   âœ“ ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ{i+1}éŸ³å£°ç”Ÿæˆå®Œäº†")

            if audio_segments and len(audio_segments) == len(narration_segments):
                from moviepy import concatenate_audioclips
                narration_audio = concatenate_audioclips(audio_segments)
                print(f"âœ“ ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³éŸ³å£°ç”Ÿæˆå®Œäº†: {narration_audio.duration:.2f}ç§’")
            else:
                narration_audio = None
                print("âš ï¸  ä¸€éƒ¨ã®éŸ³å£°ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆå­—å¹•ã®ã¿è¡¨ç¤ºã•ã‚Œã¾ã™ï¼‰")

        except Exception as e:
            print(f"âš ï¸  ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³éŸ³å£°ç”Ÿæˆã‚¹ã‚­ãƒƒãƒ—: {e}")
            print("âœ“ å­—å¹•ã®ã¿ã§ç¶šè¡Œã—ã¾ã™")
            narration_audio = None
    else:
        narration_audio = None
        narration_segments = []

    # ==========================================
    # 3. ãƒ‘ãƒ³ï¼†ã‚¯ãƒ­ãƒƒãƒ—ã‚¯ãƒªãƒƒãƒ—ã‚’ä½œæˆï¼ˆãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åŒºåˆ‡ã‚Šã«åˆã‚ã›ã¦ï¼‰
    # ==========================================
    print(f"\nã€3ã€‘ãƒ‘ãƒ³ï¼†ã‚¯ãƒ­ãƒƒãƒ—ã‚¯ãƒªãƒƒãƒ—ã‚’ä½œæˆä¸­ï¼ˆå‹•ç”»ã®é•·ã•: {actual_duration:.1f}ç§’ï¼‰...")

    # ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã¨ç”»åƒã®å¯¾å¿œé–¢ä¿‚ã‚’è¨ˆç®—
    # ç”»åƒæšæ•°ã¨ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°ã‹ã‚‰è‡ªå‹•çš„ã«å‰²ã‚Šå½“ã¦
    num_images = len(selected_images)
    num_segments = len(narration_segments) if narration_segments else 1

    # å„ç”»åƒã«ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’å‡ç­‰ã«å‰²ã‚Šå½“ã¦
    segments_per_image = num_segments / num_images

    image_timings = []

    for i in range(num_images):
        # ã“ã®ç”»åƒãŒæ‹…å½“ã™ã‚‹ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ç¯„å›²ã‚’è¨ˆç®—
        start_segment = int(i * segments_per_image)
        end_segment = int((i + 1) * segments_per_image)

        if i == num_images - 1:
            # æœ€å¾Œã®ç”»åƒã¯æ®‹ã‚Šã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå…¨ã¦ã‚’æ‹…å½“
            end_segment = num_segments

        # é–‹å§‹æ™‚åˆ»ã¨é•·ã•ã‚’è¨ˆç®—
        if narration_segments:
            start_time = narration_segments[start_segment]['start']
            end_time = narration_segments[end_segment - 1]['start'] + narration_segments[end_segment - 1]['duration']
            duration = end_time - start_time
        else:
            start_time = i * (actual_duration / num_images)
            duration = actual_duration / num_images

        # 2æšç›®ä»¥é™ã®ç”»åƒã¯0.2ç§’æ—©ãé–‹å§‹ï¼ˆãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚ˆã‚Šå…ˆã«åˆ‡ã‚Šæ›¿ãˆï¼‰
        transition_advance = 0.2
        if i > 0:
            start_time = max(0, start_time - transition_advance)
            duration += transition_advance

        image_timings.append({
            'image_path': selected_images[i],
            'start_time': start_time,
            'duration': duration,
            'segments': list(range(start_segment, end_segment))
        })

        print(f"   ç”»åƒ{i+1}: ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ{start_segment+1}-{end_segment} ({duration:.2f}ç§’)")

    # ã‚¯ãƒªãƒƒãƒ—ã‚’ä½œæˆ
    video_clips = []

    for i, timing in enumerate(image_timings):
        # äº¤äº’ã«ã‚¹ãƒ©ã‚¤ãƒ‰æ–¹å‘ã‚’å¤‰ãˆã‚‹
        slide_direction = 'left' if i % 2 == 0 else 'right'

        clip = create_slide_transition_clip(
            timing['image_path'],
            timing['duration'],
            resolution=resolution,
            slide_direction=slide_direction
        )

        # é–‹å§‹æ™‚åˆ»ã‚’è¨­å®š
        clip = clip.with_start(timing['start_time'])
        video_clips.append(clip)

        print(f"   {i+1}/{num_images}: {timing['image_path'].name} ({timing['start_time']:.2f}ç§’ã‹ã‚‰{timing['duration']:.2f}ç§’é–“ã€slide from {slide_direction})")

    # CompositeVideoClipã§åˆæˆ
    main_video = CompositeVideoClip(video_clips, size=resolution)
    main_video = main_video.with_duration(actual_duration)

    print(f"âœ“ ãƒ¡ã‚¤ãƒ³å‹•ç”»ä½œæˆå®Œäº†ï¼ˆ{main_video.duration:.1f}ç§’ï¼‰")

    # ==========================================
    # 4. ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åŒæœŸå­—å¹•ã‚’è¿½åŠ 
    # ==========================================
    print("\nã€4ã€‘ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åŒæœŸå­—å¹•ã‚’è¿½åŠ ä¸­...")

    overlays = []

    if narration_segments:
        for segment in narration_segments:
            subtitle = create_subtitle_clip_vertical(
                text=segment['text'],
                start_time=segment['start'],
                duration=segment['duration'],
                fontsize=45,
                size=resolution
            )
            overlays.append(subtitle)

        print(f"âœ“ å­—å¹•è¿½åŠ å®Œäº†: {len(overlays)}å€‹")
    else:
        print("âš ï¸  ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæœªæŒ‡å®š")

    # ==========================================
    # 5. BGMã‚’è¿½åŠ 
    # ==========================================
    bgm_audio = None

    if bgm_path and Path(bgm_path).exists():
        print("\nã€5ã€‘BGMã‚’è¿½åŠ ä¸­...")

        try:
            bgm_audio = AudioFileClip(bgm_path)

            if bgm_audio.duration > actual_duration:
                bgm_audio = bgm_audio.subclipped(0, actual_duration)

            # BGMéŸ³é‡ã‚’èª¿æ•´ï¼ˆ15%ã«ä¸‹ã’ã‚‹ï¼‰
            bgm_audio = bgm_audio.with_volume_scaled(0.15)

            print(f"âœ“ BGMè¿½åŠ å®Œäº†: {bgm_path}")

        except Exception as e:
            print(f"âš ï¸  BGMè¿½åŠ å¤±æ•—: {e}")
            bgm_audio = None
    else:
        print("\nã€5ã€‘BGMã‚’ã‚¹ã‚­ãƒƒãƒ—")

    # ==========================================
    # 6. æœ€çµ‚åˆæˆ
    # ==========================================
    print("\nã€6ã€‘æœ€çµ‚åˆæˆä¸­...")

    # ãƒ“ãƒ‡ã‚ªã‚¯ãƒªãƒƒãƒ—ã‚’åˆæˆ
    video_clips_final = [main_video] + overlays
    final_video = CompositeVideoClip(video_clips_final, size=resolution)
    final_video = final_video.with_duration(actual_duration)

    # ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚’åˆæˆ
    audio_clips = []
    if narration_audio:
        audio_clips.append(narration_audio)
    if bgm_audio:
        audio_clips.append(bgm_audio)

    if audio_clips:
        from moviepy import CompositeAudioClip
        final_audio = CompositeAudioClip(audio_clips)
        final_video = final_video.with_audio(final_audio)

    # ==========================================
    # 7. å‡ºåŠ›
    # ==========================================
    print("\nã€7ã€‘å‹•ç”»ã‚’å‡ºåŠ›ä¸­...")

    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    final_video.write_videofile(
        str(output_path),
        fps=24,
        codec='libx264',
        audio_codec='aac' if audio_clips else None,
        preset='medium',
        bitrate='5000k'
    )

    print("\n" + "=" * 60)
    print("âœ… å®Œæˆï¼")
    print("=" * 60)
    print(f"ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_path}")
    print(f"â±ï¸  å‹•ç”»ã®é•·ã•: {actual_duration:.2f}ç§’")
    print(f"ğŸ“ è§£åƒåº¦: {resolution[0]}x{resolution[1]} (ç¸¦å‹ 9:16)")
    print(f"ğŸ¬ ç”»åƒæšæ•°: {len(selected_images)}æš")
    print(f"ğŸ“ å­—å¹•: {len(overlays)}å€‹ (ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åŒæœŸ)")
    print(f"ğŸ™ï¸  ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: {'ã‚ã‚Š' if narration_audio else 'ãªã—'}")
    print(f"ğŸµ BGM: {'ã‚ã‚Š' if bgm_audio else 'ãªã—'}")
    print(f"âœ¨ ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³: ãƒ‘ãƒ³ï¼†ã‚¯ãƒ­ãƒƒãƒ—ï¼ˆã‚†ã£ãã‚Šï¼‰")
    print("=" * 60)


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""

    # è¨­å®š
    image_dir = "data/ã€ã‚ã®æˆ¦äº‰ã¯ä½•ã ã£ãŸã®ã‹ã€/images"
    output_path = "data/output/ano_senso_vertical_slide_12s.mp4"

    # ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚»ã‚°ãƒ¡ãƒ³ãƒˆï¼ˆéŸ³å£°ã®é•·ã•ã«åˆã‚ã›ã¦è‡ªå‹•èª¿æ•´ï¼‰
    narration_segments = [
        {
            "text": "æ—¥æœ¬ã¯ã©ã“ã§é–“é•ãˆãŸã®ã‹?",
            "start": 0.0,
            "duration": 3.0
        },
        {
            "text": "æ²ã’ãŸç†æƒ³ã¯ã™ã¹ã¦èª¤ã‚Šã ã£ãŸã®ã‹?",
            "start": 3.0,
            "duration": 3.0
        },
        {
            "text": "ã€Œå¤§æ±äºœã€ã¯æ—¥æœ¬ã‚’ã©ã†è¦‹ã¦ã„ãŸã‹?",
            "start": 6.0,
            "duration": 3.0
        },
        {
            "text": "æˆ¦å¾Œ80å¹´ã€ä»Šã“ãå•ã„ç›´ã™",
            "start": 9.0,
            "duration": 2.0
        },
        {
            "text": "ã€Œç§ãŸã¡ã«ã¨ã£ã¦ã®æˆ¦äº‰ã€ã¨ã¯ã€‚",
            "start": 11.0,
            "duration": 2.0
        }
    ]

    # BGMãƒ‘ã‚¹
    bgm_path = "data/bgm/yoiyaminoseaside.mp3"

    # å‹•ç”»ç”Ÿæˆ
    create_war_vertical_slide_12s(
        image_dir=image_dir,
        output_path=output_path,
        narration_segments=narration_segments,
        bgm_path=bgm_path,
        duration=12.0,
        resolution=(1080, 1920)  # ç¸¦å‹ 9:16
    )


if __name__ == "__main__":
    main()
