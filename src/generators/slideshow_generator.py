#!/usr/bin/env python3
"""
æ±ç”¨ã‚¹ãƒ©ã‚¤ãƒ‰ã‚·ãƒ§ãƒ¼å‹•ç”»ç”Ÿæˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

ç”»åƒãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆã‹ã‚‰ç¸¦å‹ã‚¹ãƒ©ã‚¤ãƒ‰ã‚·ãƒ§ãƒ¼å‹•ç”»ã‚’ç”Ÿæˆ
"""
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import shutil
import tempfile

from moviepy import (
    ImageClip,
    CompositeVideoClip,
    AudioFileClip
)
from PIL import Image, ImageDraw, ImageFont
import numpy as np

# Text-to-Speechã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from .text_to_speech_client import TextToSpeechClient


def create_slide_transition_clip(
    image_path: Path,
    duration: float,
    resolution: tuple = (1080, 1920),
    slide_direction: str = 'left',
    pan_enabled: bool = True,
    pan_scale: float = 1.15
):
    """
    æ¨ªã‚¹ãƒ©ã‚¤ãƒ‰ãƒˆãƒ©ãƒ³ã‚¸ã‚·ãƒ§ãƒ³ + æ¨ªãƒ‘ãƒ³åŠ¹æœä»˜ãã®ç”»åƒã‚¯ãƒªãƒƒãƒ—ã‚’ä½œæˆ

    Args:
        image_path: ç”»åƒãƒ‘ã‚¹
        duration: è¡¨ç¤ºæ™‚é–“
        resolution: ç¸¦å‹è§£åƒåº¦ (width, height) = (1080, 1920)
        slide_direction: ã‚¹ãƒ©ã‚¤ãƒ‰æ–¹å‘ ('left' = å·¦ã‹ã‚‰å³ã¸, 'right' = å³ã‹ã‚‰å·¦ã¸)
        pan_enabled: ãƒ‘ãƒ³åŠ¹æœã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹
        pan_scale: ãƒ‘ãƒ³ç”¨ã®å¹…å€ç‡

    Returns:
        ã‚¹ãƒ©ã‚¤ãƒ‰åŠ¹æœ + ãƒ‘ãƒ³åŠ¹æœä»˜ãImageClip
    """
    target_width, target_height = resolution

    # ãƒ‘ãƒ³åŠ¹æœç”¨ã®å¹…
    pan_width = int(target_width * pan_scale) if pan_enabled else target_width
    max_pan_offset = pan_width - target_width

    # ç”»åƒã‚’èª­ã¿è¾¼ã‚“ã§ãƒªã‚µã‚¤ã‚º
    with Image.open(image_path) as img:
        # RGBå¤‰æ›
        if img.mode != 'RGB':
            img = img.convert('RGB')

        img_width, img_height = img.size
        img_aspect = img_width / img_height

        # ç¸¦å‹ã«ãƒ•ã‚£ãƒƒãƒˆï¼ˆé«˜ã•åŸºæº–ï¼‰
        new_height = target_height
        new_width = int(new_height * img_aspect)

        # ãƒ‘ãƒ³ç”¨ã®åºƒã‚ã®å¹…ã‚’ç¢ºä¿
        if new_width < pan_width:
            new_width = pan_width
            new_height = int(new_width / img_aspect)

        # ãƒªã‚µã‚¤ã‚º
        resized_img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)

        # ä¸­å¤®ã§ã‚¯ãƒ­ãƒƒãƒ—ï¼ˆåºƒã‚ã«ï¼‰
        left = (new_width - pan_width) // 2
        top = (new_height - target_height) // 2
        cropped_img = resized_img.crop((left, top, left + pan_width, top + target_height))

        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.jpg')
        cropped_img.save(temp_file.name, 'JPEG', quality=95)
        temp_file.close()

    # ãƒˆãƒ©ãƒ³ã‚¸ã‚·ãƒ§ãƒ³æ™‚é–“ï¼ˆ0.3ç§’ï¼‰
    transition_duration = 0.3

    def slide_and_pan_effect(get_frame, t):
        """ã‚¹ãƒ©ã‚¤ãƒ‰ã‚¤ãƒ³ + ãƒ‘ãƒ³åŠ¹æœ"""
        frame = get_frame(t)

        if t < transition_duration:
            # ã‚¹ãƒ©ã‚¤ãƒ‰ã‚¤ãƒ³ä¸­
            progress = t / transition_duration
            if slide_direction == 'left':
                # å·¦ã‹ã‚‰å³ã¸ã‚¹ãƒ©ã‚¤ãƒ‰ã‚¤ãƒ³
                x_offset = int(target_width * (1 - progress))
                result = np.zeros((target_height, target_width, 3), dtype=np.uint8)
                # åºƒã„ç”»åƒã®å·¦ç«¯ã‹ã‚‰åˆ‡ã‚Šå‡ºã—
                result[:, x_offset:] = frame[:, :target_width-x_offset]
                return result
            else:
                # å³ã‹ã‚‰å·¦ã¸ã‚¹ãƒ©ã‚¤ãƒ‰ã‚¤ãƒ³
                x_offset = int(target_width * (1 - progress))
                result = np.zeros((target_height, target_width, 3), dtype=np.uint8)
                # åºƒã„ç”»åƒã®å³ç«¯ã‹ã‚‰åˆ‡ã‚Šå‡ºã—
                result[:, :target_width-x_offset] = frame[:, max_pan_offset+x_offset:max_pan_offset+target_width]
                return result
        else:
            # ãƒ‘ãƒ³åŠ¹æœï¼ˆãƒˆãƒ©ãƒ³ã‚¸ã‚·ãƒ§ãƒ³å¾Œï¼‰
            if pan_enabled and max_pan_offset > 0:
                pan_progress = (t - transition_duration) / (duration - transition_duration)

                if slide_direction == 'left':
                    # å·¦ã‹ã‚‰å³ã¸ã‚†ã£ãã‚Šãƒ‘ãƒ³
                    pan_offset = int(max_pan_offset * pan_progress)
                    return frame[:, pan_offset:pan_offset+target_width]
                else:
                    # å³ã‹ã‚‰å·¦ã¸ã‚†ã£ãã‚Šãƒ‘ãƒ³
                    pan_offset = int(max_pan_offset * (1 - pan_progress))
                    return frame[:, pan_offset:pan_offset+target_width]
            else:
                # ãƒ‘ãƒ³ãªã— - ä¸­å¤®è¡¨ç¤º
                return frame

    # åŸºæœ¬ã‚¯ãƒªãƒƒãƒ—ã‚’ä½œæˆ
    clip = ImageClip(temp_file.name).with_duration(duration)

    # ã‚¹ãƒ©ã‚¤ãƒ‰ + ãƒ‘ãƒ³åŠ¹æœã‚’é©ç”¨
    clip = clip.transform(slide_and_pan_effect)

    return clip


def create_subtitle_clip_vertical(
    text: str,
    start_time: float,
    duration: float,
    fontsize: int = 48,
    size: tuple = (1080, 1920)
):
    """
    ç¸¦å‹å‹•ç”»ç”¨ã®å­—å¹•ã‚¯ãƒªãƒƒãƒ—ã‚’ä½œæˆ

    Args:
        text: å­—å¹•ãƒ†ã‚­ã‚¹ãƒˆ
        start_time: é–‹å§‹æ™‚åˆ»
        duration: è¡¨ç¤ºæ™‚é–“
        fontsize: ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚º
        size: ç”»åƒã‚µã‚¤ã‚º

    Returns:
        ImageClip
    """
    # é€æ˜èƒŒæ™¯ã®ç”»åƒã‚’ä½œæˆ
    img = Image.new('RGBA', size, (0, 0, 0, 0))
    draw = ImageDraw.Draw(img)

    # ãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
    try:
        font = ImageFont.truetype("/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒè§’ã‚´ã‚·ãƒƒã‚¯ W6.ttc", fontsize)
    except:
        try:
            font = ImageFont.truetype("/System/Library/Fonts/Hiragino Sans GB.ttc", fontsize)
        except:
            font = ImageFont.load_default()

    # ãƒ†ã‚­ã‚¹ãƒˆã®å¢ƒç•Œãƒœãƒƒã‚¯ã‚¹ã‚’å–å¾—
    bbox = draw.textbbox((0, 0), text, font=font)
    text_width = bbox[2] - bbox[0]
    text_height = bbox[3] - bbox[1]

    # ä½ç½®ã‚’è¨ˆç®—ï¼ˆä¸‹éƒ¨ä¸­å¤®ï¼‰
    x = (size[0] - text_width) // 2
    y = size[1] - 250 - text_height

    # é»’ã„ç¸å–ã‚Šï¼ˆå¤ªã‚ï¼‰
    for offset_x in range(-4, 5):
        for offset_y in range(-4, 5):
            if offset_x != 0 or offset_y != 0:
                draw.text((x + offset_x, y + offset_y), text, font=font, fill=(0, 0, 0, 255))

    # ãƒ†ã‚­ã‚¹ãƒˆã‚’æç”»ï¼ˆç™½ï¼‰
    draw.text((x, y), text, font=font, fill=(255, 255, 255, 255))

    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.png')
    img.save(temp_file.name)
    temp_file.close()

    # ImageClipã¨ã—ã¦è¿”ã™
    return (ImageClip(temp_file.name, transparent=True)
            .with_duration(duration)
            .with_start(start_time))


def generate_slideshow(
    image_paths: List[Path],
    output_path: Path,
    narration_segments: Optional[List[Dict]] = None,
    bgm_path: Optional[Path] = None,
    duration: float = 12.0,
    resolution: Tuple[int, int] = (1080, 1920),
    transition_advance: float = 0.2,
    pan_enabled: bool = True,
    pan_scale: float = 1.15,
    enable_tts: bool = True
):
    """
    ç”»åƒãƒªã‚¹ãƒˆã‹ã‚‰ã‚¹ãƒ©ã‚¤ãƒ‰ã‚·ãƒ§ãƒ¼å‹•ç”»ã‚’ç”Ÿæˆ

    Args:
        image_paths: ç”»åƒãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        output_path: å‡ºåŠ›ãƒ‘ã‚¹
        narration_segments: ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ
        bgm_path: BGMãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        duration: å‹•ç”»ã®é•·ã•
        resolution: è§£åƒåº¦
        transition_advance: åˆ‡ã‚Šæ›¿ãˆã‚’ä½•ç§’æ—©ãã™ã‚‹ã‹
        pan_enabled: ãƒ‘ãƒ³åŠ¹æœã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹
        pan_scale: ãƒ‘ãƒ³ç”¨ã®å¹…å€ç‡
        enable_tts: TTSã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹

    Returns:
        å‡ºåŠ›ãƒ‘ã‚¹
    """
    print("=" * 60)
    print("ğŸ¬ ã‚¹ãƒ©ã‚¤ãƒ‰ã‚·ãƒ§ãƒ¼å‹•ç”»ç”Ÿæˆ")
    print("=" * 60)

    # ==========================================
    # 1. ç”»åƒã‚’æº–å‚™
    # ==========================================
    print(f"\nã€1ã€‘ç”»åƒã‚’æº–å‚™ä¸­... ({len(image_paths)}æš)")

    if len(image_paths) < 1:
        raise ValueError("å°‘ãªãã¨ã‚‚1æšã®ç”»åƒãŒå¿…è¦ã§ã™")

    # ==========================================
    # 2. ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³éŸ³å£°ã‚’ç”Ÿæˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    # ==========================================
    updated_narration_segments = []
    narration_audio = None
    actual_duration = duration

    if narration_segments:
        print(f"\nã€2ã€‘å­—å¹•ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’æº–å‚™ä¸­... ({len(narration_segments)}å€‹)")

        # éŸ³å£°åˆæˆã‚’è©¦ã¿ã‚‹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        audio_segments = []
        if enable_tts:
            try:
                print("\nã€2-1ã€‘ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³éŸ³å£°ã‚’ç”Ÿæˆä¸­...")
                tts_client = TextToSpeechClient()

                for i, segment in enumerate(narration_segments):
                    result = tts_client.synthesize_speech(
                        text=segment['text'],
                        output_name=f"slideshow_narration_segment_{i+1}",
                        language_code="ja-JP",
                        voice_name=tts_client.JAPANESE_VOICES["male_a"],
                        voice_gender="MALE",
                        speaking_rate=1.2,
                        pitch=-5.0,
                        volume_gain_db=3.0,
                        output_dir=Path("data/output/speech")
                    )

                    if result['status'] == 'success':
                        audio_clip = AudioFileClip(str(result['audio_file']))
                        audio_segments.append(audio_clip)
                        print(f"   âœ“ ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ{i+1}éŸ³å£°ç”Ÿæˆå®Œäº†: {audio_clip.duration:.2f}ç§’")
                    else:
                        print(f"   âœ— ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ{i+1}éŸ³å£°ç”Ÿæˆå¤±æ•—")

                if audio_segments and len(audio_segments) == len(narration_segments):
                    from moviepy import concatenate_audioclips
                    narration_audio = concatenate_audioclips(audio_segments)
                    print(f"âœ“ ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³éŸ³å£°ç”Ÿæˆå®Œäº†: {narration_audio.duration:.2f}ç§’")
                else:
                    print("âš ï¸  ä¸€éƒ¨ã®éŸ³å£°ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆå­—å¹•ã®ã¿è¡¨ç¤ºã•ã‚Œã¾ã™ï¼‰")
                    audio_segments = []

            except Exception as e:
                print(f"âš ï¸  ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³éŸ³å£°ç”Ÿæˆã‚¹ã‚­ãƒƒãƒ—: {e}")
                print("âœ“ å­—å¹•ã®ã¿ã§ç¶šè¡Œã—ã¾ã™")
                audio_segments = []

        # å­—å¹•ç”¨ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’æº–å‚™ï¼ˆå®Ÿéš›ã®TTSé•·ã«åŸºã¥ãï¼‰
        print("\nã€2-2ã€‘å­—å¹•ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚’èª¿æ•´ä¸­...")
        current_time = 0.0

        if audio_segments and len(audio_segments) == len(narration_segments):
            # TTSéŸ³å£°ã®å®Ÿéš›ã®é•·ã•ã«åŸºã¥ã„ã¦èª¿æ•´
            print("   â†’ TTSéŸ³å£°ã®å®Ÿéš›ã®é•·ã•ã«åŸºã¥ã„ã¦èª¿æ•´")
            for i, segment in enumerate(narration_segments):
                actual_audio_duration = audio_segments[i].duration
                updated_segment = {
                    'text': segment['text'],
                    'start': current_time,
                    'duration': actual_audio_duration
                }
                updated_narration_segments.append(updated_segment)
                print(f"   ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ{i+1}: {actual_audio_duration:.2f}ç§’ - {segment['text']}")
                current_time += actual_audio_duration

            # å®Ÿéš›ã®é•·ã•ã‚’æ›´æ–°
            actual_duration = current_time
            print(f"âœ“ å®Ÿéš›ã®å‹•ç”»ã®é•·ã•: {actual_duration:.2f}ç§’")
        else:
            # TTSéŸ³å£°ãªã—ï¼šå‡ç­‰ã«åˆ†å‰²
            print("   â†’ å‡ç­‰åˆ†å‰²ï¼ˆTTSéŸ³å£°ãªã—ï¼‰")
            segment_duration = duration / len(narration_segments)
            for i, segment in enumerate(narration_segments):
                updated_segment = {
                    'text': segment['text'],
                    'start': current_time,
                    'duration': segment_duration
                }
                updated_narration_segments.append(updated_segment)
                print(f"   ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ{i+1}: {segment_duration:.2f}ç§’ - {segment['text']}")
                current_time += segment_duration

        narration_segments = updated_narration_segments
        print(f"âœ“ å­—å¹•ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæº–å‚™å®Œäº†: {len(narration_segments)}å€‹")

    # ==========================================
    # 3. ç”»åƒã‚¯ãƒªãƒƒãƒ—ã‚’ä½œæˆ
    # ==========================================
    print(f"\nã€3ã€‘ç”»åƒã‚¯ãƒªãƒƒãƒ—ã‚’ä½œæˆä¸­ï¼ˆå‹•ç”»ã®é•·ã•: {actual_duration:.1f}ç§’ï¼‰...")

    # ç”»åƒã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚’è¨ˆç®—
    num_images = len(image_paths)
    num_segments = len(narration_segments) if narration_segments else 1

    # å„ç”»åƒã«ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’å‡ç­‰ã«å‰²ã‚Šå½“ã¦
    segments_per_image = num_segments / num_images

    image_timings = []

    for i in range(num_images):
        # ã“ã®ç”»åƒãŒæ‹…å½“ã™ã‚‹ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ç¯„å›²ã‚’è¨ˆç®—
        start_segment = int(i * segments_per_image)
        end_segment = int((i + 1) * segments_per_image)

        if i == num_images - 1:
            # æœ€å¾Œã®ç”»åƒã¯æ®‹ã‚Šã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå…¨ã¦ã‚’æ‹…å½“
            end_segment = num_segments

        # é–‹å§‹æ™‚åˆ»ã¨é•·ã•ã‚’è¨ˆç®—
        if narration_segments:
            start_time = narration_segments[start_segment]['start']
            end_time = narration_segments[end_segment - 1]['start'] + narration_segments[end_segment - 1]['duration']
            img_duration = end_time - start_time
        else:
            start_time = i * (actual_duration / num_images)
            img_duration = actual_duration / num_images

        # 2æšç›®ä»¥é™ã®ç”»åƒã¯æ—©ãé–‹å§‹ï¼ˆãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚ˆã‚Šå…ˆã«åˆ‡ã‚Šæ›¿ãˆï¼‰
        if i > 0:
            start_time = max(0, start_time - transition_advance)
            img_duration += transition_advance

        image_timings.append({
            'image_path': image_paths[i],
            'start_time': start_time,
            'duration': img_duration,
            'segments': list(range(start_segment, end_segment))
        })

        print(f"   ç”»åƒ{i+1}: ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ{start_segment+1}-{end_segment} ({img_duration:.2f}ç§’)")

    # ã‚¯ãƒªãƒƒãƒ—ã‚’ä½œæˆ
    video_clips = []

    for i, timing in enumerate(image_timings):
        # äº¤äº’ã«ã‚¹ãƒ©ã‚¤ãƒ‰æ–¹å‘ã‚’å¤‰ãˆã‚‹
        slide_direction = 'left' if i % 2 == 0 else 'right'

        clip = create_slide_transition_clip(
            timing['image_path'],
            timing['duration'],
            resolution=resolution,
            slide_direction=slide_direction,
            pan_enabled=pan_enabled,
            pan_scale=pan_scale
        )

        # é–‹å§‹æ™‚åˆ»ã‚’è¨­å®š
        clip = clip.with_start(timing['start_time'])
        video_clips.append(clip)

        print(f"   {i+1}/{num_images}: {timing['image_path'].name} "
              f"({timing['start_time']:.2f}ç§’ã‹ã‚‰{timing['duration']:.2f}ç§’é–“ã€slide from {slide_direction})")

    # CompositeVideoClipã§åˆæˆ
    main_video = CompositeVideoClip(video_clips, size=resolution)
    main_video = main_video.with_duration(actual_duration)

    print(f"âœ“ ãƒ¡ã‚¤ãƒ³å‹•ç”»ä½œæˆå®Œäº†ï¼ˆ{main_video.duration:.1f}ç§’ï¼‰")

    # ==========================================
    # 4. ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åŒæœŸå­—å¹•ã‚’è¿½åŠ 
    # ==========================================
    print("\nã€4ã€‘ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åŒæœŸå­—å¹•ã‚’è¿½åŠ ä¸­...")

    overlays = []

    if narration_segments:
        for segment in narration_segments:
            subtitle = create_subtitle_clip_vertical(
                text=segment['text'],
                start_time=segment['start'],
                duration=segment['duration'],
                fontsize=45,
                size=resolution
            )
            overlays.append(subtitle)

        print(f"âœ“ å­—å¹•è¿½åŠ å®Œäº†: {len(overlays)}å€‹")
    else:
        print("âš ï¸  ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæœªæŒ‡å®š")

    # ==========================================
    # 5. BGMã‚’è¿½åŠ 
    # ==========================================
    bgm_audio = None

    if bgm_path and bgm_path.exists():
        print("\nã€5ã€‘BGMã‚’è¿½åŠ ä¸­...")

        try:
            bgm_audio = AudioFileClip(str(bgm_path))

            if bgm_audio.duration > actual_duration:
                bgm_audio = bgm_audio.subclipped(0, actual_duration)

            # BGMéŸ³é‡ã‚’èª¿æ•´ï¼ˆ15%ã«ä¸‹ã’ã‚‹ï¼‰
            bgm_audio = bgm_audio.with_volume_scaled(0.15)

            print(f"âœ“ BGMè¿½åŠ å®Œäº†: {bgm_path}")

        except Exception as e:
            print(f"âš ï¸  BGMè¿½åŠ å¤±æ•—: {e}")
            bgm_audio = None
    else:
        print("\nã€5ã€‘BGMã‚’ã‚¹ã‚­ãƒƒãƒ—")

    # ==========================================
    # 6. æœ€çµ‚åˆæˆ
    # ==========================================
    print("\nã€6ã€‘æœ€çµ‚åˆæˆä¸­...")

    # ãƒ“ãƒ‡ã‚ªã‚¯ãƒªãƒƒãƒ—ã‚’åˆæˆ
    video_clips_final = [main_video] + overlays
    final_video = CompositeVideoClip(video_clips_final, size=resolution)
    final_video = final_video.with_duration(actual_duration)

    # ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚’åˆæˆ
    audio_clips = []
    if narration_audio:
        audio_clips.append(narration_audio)
    if bgm_audio:
        audio_clips.append(bgm_audio)

    if audio_clips:
        from moviepy import CompositeAudioClip
        final_audio = CompositeAudioClip(audio_clips)
        final_video = final_video.with_audio(final_audio)

    # ==========================================
    # 7. å‡ºåŠ›
    # ==========================================
    print("\nã€7ã€‘å‹•ç”»ã‚’å‡ºåŠ›ä¸­...")

    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    final_video.write_videofile(
        str(output_path),
        fps=24,
        codec='libx264',
        audio_codec='aac' if audio_clips else None,
        preset='medium',
        bitrate='5000k'
    )

    print("\n" + "=" * 60)
    print("âœ… å®Œæˆï¼")
    print("=" * 60)
    print(f"ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_path}")
    print(f"â±ï¸  å‹•ç”»ã®é•·ã•: {actual_duration:.2f}ç§’")
    print(f"ğŸ“ è§£åƒåº¦: {resolution[0]}x{resolution[1]}")
    print(f"ğŸ¬ ç”»åƒæšæ•°: {num_images}æš")
    print(f"ğŸ“ å­—å¹•: {len(overlays)}å€‹")
    print(f"ğŸ™ï¸  ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: {'ã‚ã‚Š' if narration_audio else 'ãªã—'}")
    print(f"ğŸµ BGM: {'ã‚ã‚Š' if bgm_audio else 'ãªã—'}")
    print(f"âœ¨ ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³: æ¨ªã‚¹ãƒ©ã‚¤ãƒ‰ + {'ãƒ‘ãƒ³' if pan_enabled else 'ãƒ‘ãƒ³ãªã—'}")
    print("=" * 60)

    return output_path
